# =======================================================
# Crawl4AI PostgreSQL Database Configuration
# =======================================================
# Copy this file to .env and fill in your database details
# The crawl4ai project will automatically use these settings
# =======================================================

# ----- SUPABASE CONFIGURATION (Recommended) -----
# If you're using Supabase, configure these variables:

# Your Supabase project URL (e.g., https://xxxxxxxxxxxx.supabase.co)
SUPABASE_URL=

# Your Supabase project's database password (from project settings)
SUPABASE_PASSWORD=

# Optional: Supabase service role key (for administrative operations)
SUPABASE_ANON_KEY=

# Optional: Customize connection settings
SUPABASE_USER=postgres
SUPABASE_DB=postgres
SUPABASE_PORT=5432
SUPABASE_SCHEMA=crawl4ai
SUPABASE_MIN_CONN=1
SUPABASE_MAX_CONN=10

# ----- GENERIC POSTGRESQL CONFIGURATION -----
# If you're using a different PostgreSQL provider, use these:

# Database host (e.g., localhost, your-db-host.com)
POSTGRES_HOST=localhost

# Database port (default: 5432)
POSTGRES_PORT=5432

# Database name
POSTGRES_DB=crawl4ai

# Database username
POSTGRES_USER=postgres

# Database password
POSTGRES_PASSWORD=

# Database schema (default: crawl4ai)
POSTGRES_SCHEMA=crawl4ai

# Connection pool settings
POSTGRES_MIN_CONN=1
POSTGRES_MAX_CONN=20

# ----- BACKWARD COMPATIBILITY -----
# These variables are also supported for compatibility with existing setups:

# DB_HOST=localhost
# DB_PORT=5432
# DB_NAME=crawl4ai
# DB_USER=postgres
# DB_PASSWORD=

# =======================================================
# USAGE INSTRUCTIONS
# =======================================================
# 
# 1. SUPABASE SETUP:
#    - Create a new project at https://supabase.com
#    - Go to Settings > Database
#    - Copy your database URL and password
#    - Fill in SUPABASE_URL and SUPABASE_PASSWORD above
#
# 2. GENERIC POSTGRESQL SETUP:
#    - Fill in your PostgreSQL connection details
#    - Make sure the database exists and is accessible
#    - The crawl4ai schema and tables will be created automatically
#
# 3. RUNNING MIGRATIONS:
#    - Migrations run automatically when the application starts
#    - You can also run them manually:
#      python -c "import asyncio; from crawl4ai import run_postgres_migrations; asyncio.run(run_postgres_migrations())"
#
# 4. TESTING CONNECTION:
#    - Test your connection:
#      python -c "import asyncio; from crawl4ai.postgres_config import PostgreSQLConfig; from crawl4ai.postgres_database import get_postgres_db_manager; asyncio.run(get_postgres_db_manager().initialize())"
#
# =======================================================
# SCHEMA INFORMATION
# =======================================================
# 
# The following tables will be created automatically:
# - crawl4ai.crawl_metadata: Stores crawled page metadata
# - crawl4ai.event_codes: Windows Event code information
# - crawl4ai.event_descriptions: Human-readable event descriptions
# - crawl4ai.event_logs: Event log entries
# - crawl4ai.event_references: External references for events
# - crawl4ai.migrations: Migration tracking
#
# Views:
# - crawl4ai.event_codes_with_descriptions: Combined event data
#
# ======================================================= 